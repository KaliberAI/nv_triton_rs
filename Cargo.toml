[package]
name = "nv_triton_rs"
version = "0.1.0"
edition = "2024"
description = "Rust client for NVIDIA Triton Inference Server"
readme = "README.md"
repository = "https://github.com/KaliberAI/nv_triton_rs"
license = "Apache-2.0"
keywords = ["machine-learning", "ml", "triton"]
categories = ["api-bindings"]
exclude = ["common/"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
tonic = "0.13"
prost = "0.13"
tokio = { version = "1", features = ["macros", "rt-multi-thread"] }
thiserror = "1"
tracing = "0.1"

[dev-dependencies]
tracing-test = "0.2"

[build-dependencies]
tonic-build = "0.13"

[lib]
crate-type = ["lib"]

[[example]]
name = "load_unload_model"
path = "examples/load_unload_model.rs"
